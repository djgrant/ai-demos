[
  {
    "title": "Models Megathread #4 - What models are you currently using?",
    "url": "https://www.reddit.com/r/LocalLLaMA/comments/1bgfttn/models_megathread_4_what_models_are_you_currently/",
    "comments": [
      "I test models on a wide range of devices, so I have several favorites depending on the size.\n\n## General-purpose models\n- **1.1B:** [TinyDolphin 2.8 1.1B](https://huggingface.co/Crataco/TinyDolphin-2.8-1.1b-imatrix-GGUF). Takes about ~700MB RAM and tested on my Pi 4 with 2 gigs of RAM. Hallucinates a lot, but works for basic conversation.\n- **2.7B:** [Dolphin 2.6 Phi-2](https://huggingface.co/TheBloke/dolphin-2_6-phi-2-GGUF). Takes over ~2GB RAM and tested on my 3GB 32-bit phone via llama.cpp on Termux. \n- **7B:** [Nous Hermes Mistral 7B DPO](https://huggingface.co/Crataco/Nous-Hermes-2-Mistral-7B-DPO-imatrix-GGUF). Takes about ~4-5GB RAM depending on context length. Works on my laptop with 8GB RAM.\n- **10.7B:** [Nous Hermes 2 SOLAR 10.7B](https://huggingface.co/TheBloke/Nous-Hermes-2-SOLAR-10.7B-GGUF). Takes about ~6-8GB RAM depending on context length. Works on my server PCs and my primary PC (16GB RAM, 4GB VRAM).\n- **8x7B:** [Nous Hermes 2 Mixtral 8x7B DPO](https://huggingface.co/mradermacher/Nous-Hermes-2-Mixtral-8x7B-DPO-i1-GGUF). At IQ3_S, it can run on a laptop with 16GB RAM and 8GB VRAM with 10-11 layers offloaded at 4096 ctx, but I recall it's slightly slower than Q3_K_S (which I had a more consistent ~4.4 tokens/sec with).\n\n## Roleplay models\n- **7B:** [Kunoichi-DPO-v2-7B](https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF) is my most reliable, but I **love** [Erosumika](https://huggingface.co/Lewdiculous/Erosumika-7B-GGUF-IQ-Imatrix), which sacrifices the logical yet synthetic GPT dataset for something more organic, similar to older models like Noromaid, Pygmalion 6B, and AI Dungeon 2.\n- **10.7B:** [Fimbulvetr-11B-v2](https://huggingface.co/mradermacher/Fimbulvetr-11B-v2-i1-GGUF). I haven't tested it nearly as much as Kunoichi, so I can't vouch for it, but I hear a lot of great things about it!\n- **8x7B:** [BagelMIsteryTour-v2-8x7B](https://huggingface.co/ycros/BagelMIsteryTour-v2-8x7B-GGUF), probably the best RP model I've ever ran since it hits a great balance of prose and intelligence.\n\nAnd some extra information, while I'm at it:\n\n## Frontends\n- [ollama](https://github.com/ollama/ollama): terminal client. I use it for quick Q&A. Automatically offloads GPU layers, easy to download and get a model running, etc. But it [lags behind upstream llama.cpp](https://github.com/ollama/ollama/pull/1825).\n- [llama.cpp](https://github.com/ggerganov/llama.cpp): terminal client. With cmake, I was able to compile the latest commit (c47cf41) on my 32-bit Android phone.\n- [SillyTavern](https://github.com/SillyTavern/SillyTavern) with [KoboldCpp](https://github.com/LostRuins/koboldcpp): For roleplay, or just because of its fancy interface, usually if I'm using a model hosted from another PC, want to do a roleplay, or use its built-in RAG capability (vector storage).\n\n## Settings\n- **General chatting:** Min P at 0.1 (0.01 for 8x7B models), temperature at 1.0.\n- **Roleplaying (7B):** Min P at 0.1 (or 0.05 if the prose is dry), \"Smoothing Factor\" at 0.23, temperature at 2 (as last sampler, not first).\n- **Roleplaying (8x7B):** Min P at 0.001, \"Smoothing Factor\" at 0.23, temperature between 1-1.25 (as last sampler).\n\n**EDIT:** Also, if you're running your models on CPU, or maybe using GPU offloading, it's important to test and see which thread count (for prompt processing and generation) gives you the best results.",
      "Let's be real here, no small amount of attention is paid to this sub by people who are looking for lewd.  I'm as fascinated as anyone by the possibilities of how this stuff could change our world, and it is super-exciting to watch this technology evolve into a way that anyone at home could have it - it's like unboxing your first Commodore 64 all over again.\n\nBut nothing has moved technology along like our base human desires, and I am human too.\n\n**Westlake-10.7B-v2** is the newcomer to the dirty games and fits in as little as 8GB.  Almost anyone with a mid-spec gaming rig can run this well and get their fix, and competes very well with the classic 70B+ models, which is nothing short of amazing.  You could stop here and just get this one and you will leave this thread happy.\n\nAnything with Noromaid in it is a staple of rip your clothes off style raunch, a few flavors are worth mentioning.  **Noromaid 20B, EstopianMaid 13B, Noromaid-0.4-Mixtral-8x7B-ZLoss**, and the new **MiquMaid** variants will do their worst to you with even the slightest suggestion.\n\nFor a more intelligent good time with a slower burn, and if you have lots of VRAM (48GB recommended), consider **Midnight-Rose** or **Midnight-Miqu** (less smutty and more smutty, respectively), in their 70B or 103B forms.  Even at small quants, IQ2 or IQ3, they write very well, just be a little more patient.  They'll run very well on 2x RTX 3090s.\n\nAnd whatever you do, don't reply with anything else that might arouse, titillate, or seduce someone into taking an imaginary partner or thirty into their own hand.",
      "Midnight-Miqu-103B-v1.0 for creative writing, it's noticeably more intelligent then even the best 70B models.",
      "### Use case:\n\nRoleplay chatting with character cards. Small models.\n\nI mostly look for strong character card adherence, system prompt following, response formatting, general coherence and models that will just go along with the most hardcore NSFW roleplay without resistance.\n\nRecommendations are always welcome.\n\n  - Backend: KoboldCpp (`--contextsize 8192`)\n  - Frontend: SillyTavern\n\n---\n\n### Models:\n\n1) InfinityRP (7B)\n\nAn overall great model with solid character following and great response formatting. Seems to know not to write/speak for the {{user}} and when to stop.\n\n\"This model was basically made to stop some upsetting hallucinations, so {{char}} mostly and occasionally will wait {{user}} response instead of responding itself or deciding for {{user}}, also, my primary idea was to create a cozy model that thinks.\"\n\n  - Model: [Endevor/InfinityRP-v1-7B](https://huggingface.co/Endevor/InfinityRP-v1-7B)\n  - Quants: [Lewdiculous/InfinityRP-v1-7B-GGUF-IQ-Imatrix](https://huggingface.co/Lewdiculous/InfinityRP-v1-7B-GGUF-IQ-Imatrix)\n\n---\n\n2) BuRP (7B)\n\nSimilar to the above, but with more unalignment. Generally also pretty solid with a slightly different style you might like compared to the original InfinityRP.\n\nThe model card feels like a personal *attack* on my formatting complaints and I can respect that.\n\n\"So you want a model that can do it all? You've been dying to RP with a superintelligence who never refuses your advances while sticking to your strange and oddly specific dialogue format? Well, look no further because BuRP is the model you need.\"\n\n  - Model: [ChaoticNeutrals/BuRP_7B](https://huggingface.co/ChaoticNeutrals/BuRP_7B)\n  - Quants: [Lewdiculous/BuRP_7B-GGUF-IQ-Imatrix](https://huggingface.co/Lewdiculous/BuRP_7B-GGUF-IQ-Imatrix)\n\n---\n\n3) Layris (9B)\n\nThis passthrough Eris merge aimed to bring a high scoring model together with Layla-V4. It has shown to be smart and unaligned. Also a good option in this parameter size for our use case.\n\n  - Model: [ChaoticNeutrals/Layris_9B](https://huggingface.co/ChaoticNeutrals/Layris_9B/)\n  - Quants: [Lewdiculous/Layris_9B-GGUF-IQ-Imatrix](https://huggingface.co/Lewdiculous/Layris_9B-GGUF-IQ-Imatrix)\n\n---\n\n4) Infinitely-Laydiculous (7B)\n\nI really like InfinityRP's style, and wanted to see it merged with Layla-V4 for her absolute unhingedness/unalignment.\n\n  - Model: [Nitral-AI/Infinitely-Laydiculous-7B](https://huggingface.co/Nitral-AI/Infinitely-Laydiculous-7B)\n  - Quants: [Lewdiculous/Infinitely-Laydiculous-7B-GGUF-IQ-Imatrix](https://huggingface.co/Lewdiculous/Infinitely-Laydiculous-7B-GGUF-IQ-Imatrix)\n\n---\n\n5) Kunoichi-DPO-v2 (7B)\n\nGreat all around choice. Widely recommended by many users. Punches above what you'd expect.\n\n  - Model: [SanjiWatsuki/Kunoichi-DPO-v2-7B](https://huggingface.co/SanjiWatsuki/Kunoichi-DPO-v2-7B)\n  - Quants: [Lewdiculous/Kunoichi-DPO-v2-7B-GGUF-Imatrix](https://huggingface.co/Lewdiculous/Kunoichi-DPO-v2-7B-GGUF-Imatrix)\n\n---\n\n6) Layla-V4 (7B)\n\nThis model has been stripped out of all refusals. A truly based and unaligned breed that is solid for roleplaying. A NSFW natural.\n\n[I highly recommend you read this post here.](https://www.reddit.com/r/LocalLLaMA/comments/1b3jj0v)\n\n  - Model: [l3utterfly/mistral-7b-v0.1-layla-v4](https://huggingface.co/l3utterfly/mistral-7b-v0.1-layla-v4/)\n  - Quants: [Lewdiculous/mistral-7b-v0.1-layla-v4-GGUF-IQ-Imatrix](https://huggingface.co/Lewdiculous/mistral-7b-v0.1-layla-v4-GGUF-IQ-Imatrix)\n\n---\n\n7) Kunocchini (128k-test) (7B)\n\nKunoichi-DPO-v2 with better handling of longer contexts.\n\n  - Model: [Nitral-AI/Kunocchini-7b-128k-test](https://huggingface.co/Nitral-AI/Kunocchini-7b-128k-test)\n  - Quants: [Lewdiculous/Kunocchini-7b-128k-test-GGUF-Imatrix](https://huggingface.co/Lewdiculous/Kunocchini-7b-128k-test-GGUF-Imatrix)",
      "I'm obviously partial, but I've been running [wolfram/miquliz-120b-v2.0](https://huggingface.co/wolfram/miquliz-120b-v2.0) almost exclusively since making it. And I just uploaded [additional imatrix GGUF quants](https://huggingface.co/wolfram/miquliz-120b-v2.0-GGUF) today, from IQ1_S to IQ4_XS and in-between (even at 2-bit with IQ2_XS it works great).",
      "Midnight Miqu 70b 1.0/1.5 for rp, switched from Miquliz 120b.\n\nI also tried mistral\\_7b\\_instruct\\_v0.2\\_DARE with mistral-7b-mmproj-v1.5-Q4\\_1 for multimodal this week, it's repeating some stuff but overall it shown better accuracy and less hallucinations in describing images than yi-vl-34b (not sure if yi-vl is just bad or maybe I'm doing something wrong).",
      "**Mistral 7B Instruct v0.2 at Q8\\_0**\n\nThe best all-round daily task: full 32k contexts, fast, affordable, good foreign language, somewhat uncensored, and good for RAG tasks.\n\nContext size: 2k is junk, 4k is basic, 8k is decent, 16k is okay, 32k is good.\n\nI work a lot of summarizing and wisdom extraction\\*\\* of long texts around 12k-26k contexts, so 32k contexts is a must. It eats 21+ GB VRAM on my 2x3060.\n\nI'm also wanting Mixtral 8x7B, surely will be much better than Mistral 7B. But I have no idea how many GPUs to run it with 32k contexts, maybe needs 3x3090 cmiiw. So, the little brother still the best for now.\n\n\\*\\*Thanks to [Daniel Miessler](https://github.com/danielmiessler/) for the [yt.py](http://yt.py) script and the special prompt. Amazing!"
    ]
  },
  {
    "title": "Who's next?",
    "url": "https://i.redd.it/5rma8h7xqipc1.png",
    "comments": [
      "Stability",
      "The logical next victim is everyoneâ€™s sense of optimism",
      "Satya nadella is like a shadow behind most of ai startup",
      "It cannot be Claude, right?",
      "Huggingface",
      "Microsoft has a $16 million investment in Mistral at a $2 billion valuation. That is less than 1% voting shares. Mistral started their API with Mistral Medium way earlier, shortly after the $415 million funding round led by a16z. Microsoft does not control Mistral and has nothing to do with the move to closed-source models. If you want to be angry with someone, direct that anger at the a16z CEO Andreessen Horowitz.",
      "Context?"
    ]
  },
  {
    "title": "LLM Leaderboards are Bullshit - Goodhart's Law Strikes Again",
    "url": "https://www.reddit.com/r/LocalLLaMA/comments/1bjvjaf/llm_leaderboards_are_bullshit_goodharts_law/",
    "comments": [
      "The one that frustrates me is that people keep using the same stupid ass riddles to test model intelligence despite the fact that the riddles have been around long enough to become part of the training data at this point. \n\nSally's sisters isn't a valid test of model intelligence. You can find the same problem and answer solution verbatim on hundreds of sites at this point. Stop testing models with riddles.",
      "TruthfulQA: 79.84\n\n In my subjective tests, models from Jon Durbin, HF team, NousHermes, Intel and a few other well-known teams/people are the best by far. They also extensively share their graphs, workflows and datasets. Most if not all top scorers don't even share their LoRA hyperparameters. Hell, the top scoring model (smaug) doesn't even have a proper fucking instruct template written anywhere.\n\nLooking at Miqu, Zephyr and Jon's models, they all score slightly above average but they are all better than the rest objectively. I also kind of confirmed this with my experiments. I have a few Mistral models on the board that barely scored better than Zephyr, and I couldn't find a way to increase the scores that much other than cheating.\n\nOh also, again a few of my models scored slightly above average but they are slightly broken! I can't get them to process very long or very short text properly, especially in ChatML template.",
      "True, we should factor in more thing when evaluating, like extra-curriculars and letters of reccomendations.",
      "I would like to rebut that it's rare to find any researcher who will defend the leaderboards as a good or practical solution for anything. It's mostly marketing and recreational competition that focuses on them.",
      "Some form of chatbot arena for open llms would be great, but infinitely expensive to maintain",
      "A simple solution is to allow people to review models, leave a rating, etc.  Since the only benchmark we really care about is if people enjoy using it.",
      "Have a benchmark that is opaque run by a third party."
    ]
  },
  {
    "title": "I hate Microsoft",
    "url": "https://www.reddit.com/r/LocalLLaMA/comments/1bjmsfq/i_hate_microsoft/",
    "comments": [
      "Of course they want to kill the competition/keep the competition low, that's why I wrote an antitrust review request to the european commission as soon as i heard about the partnership deal with Mistral. I really want to know the result for that case, but it'll probably take a while.",
      "[https://en.wikipedia.org/wiki/Embrace,\\_extend,\\_and\\_extinguish](https://en.wikipedia.org/wiki/Embrace,_extend,_and_extinguish)",
      "Mistral with integrated Co-Pilot coming soon\\*.\n\n*^(\\* Microsoft Edge required.)*",
      "First time? They kept putting spyware in windows and now act like they own your machine.",
      "It has nothing to do with microsoft, this clearly was mistral's plan from the beginning. They followed openai's steps.",
      "Did anyone really think Mistral and others are out for social benefit alone ? Any VC funded AI company will seek a way to return to investors. They raised 385M euros. Those VC board members are asking for their $$$$ and Euros ! What is the rationale behind these type of posts ?",
      "How are they doing that? Iâ€™m no fanboy and primarily use Ubuntu but itâ€™s thanks in part to the MS billions that startups have a path to money, meaning more startups and more open source. Also, MS is publishing a lot of research and have released their own open source projects. \n\nThey could be far more evil about this, cf. OpenAI, Apple, IBM, any number of large tech companies who are clearly active in this space but sharing very little if anything at all."
    ]
  },
  {
    "title": "Japan org creates evolutionary automatic merging algorithm",
    "url": "https://www.reddit.com/r/LocalLLaMA/comments/1bk1ujz/japan_org_creates_evolutionary_automatic_merging/",
    "comments": [
      "That's pretty dang banger.\n\n> Is this something we couldn't do before? could this actually be pretty significant?\n\nPrincipally we could, but these guys implemented some defacto finetune sauce that makes it seemingly work pretty good without necessarily inflating the final weights like for instance Goliath does. It's a little more involved and task specific but could be a way to keep the models relatively small."
    ]
  },
  {
    "title": "Just got a second 3090, what models are you guys running with all this VRAM?",
    "url": "https://www.reddit.com/r/LocalLLaMA/comments/1bjx69l/just_got_a_second_3090_what_models_are_you_guys/",
    "comments": [
      "LoneStriker_miqu-1-70b-sf-5.0bpw-h6-exl2 and Midnight-Miqu-70B-v1.5_exl2_5.0bpw\n\nI use Text Gen Web UI, usually a 21,24 GPU split with 4bit cache. I'm kind of lazy with context, but usually run 16k or so. Though with 4bit cache with a 5bpw quant, 32k is doable.\n\nMiqu is just the best all around best performing model, for non-RP stuff(though I haven't tried the 120b merges of Miqu). Midnight Miqu 1.5 is solid for roleplay, though to be fair I'm always experimenting with my own 70b merges so I haven't played with Midnight too much.",
      "Good ol Mixtral for me, 6bit quant. I run it through open webui and ollama and it's great.",
      "Miquliz-120b at 2.4bpw exl2 with 2x3090 lets me have 57k context when 4bit cache is enabled",
      "Try Mixtral instruct gguf q6, or Miqu EXL2 5.0bpw",
      "SUPIR upscaling with Scable Diffusion XL images.",
      "I can't find anything I like better than Mixtral-Instruct. I asked a similar question  [here](https://www.reddit.com/r/LocalLLaMA/comments/1b9e47s/favorite_mixtral_variant/?utm_source=share&utm_medium=mweb3x&utm_name=mweb3xcss&utm_term=1&utm_content=share_button) a few weeks ago.",
      "Goliath 120b, creative prose still makes me unable to live without it.   \nEven though Miqu-120B and Miquliz-120B have appeared.\n\n  \nThe disadvantage is that the maximum context is only 4096."
    ]
  },
  {
    "title": "The Era of 1 bit LLMs - Training, Tips, Code",
    "url": "https://www.reddit.com/r/LocalLLaMA/comments/1bjinlq/the_era_of_1_bit_llms_training_tips_code/",
    "comments": [
      "Now we just need someone to train one.",
      "\"You will run the 1-bit LLM and be happy.\"  \n- Klaus Hwang, Agenda 24GB",
      "im more interested about small llms being trained on these changes than my expectations for gpt 5 from closedai",
      "I wish more researchers would publish FAQs like this.\n\nThe first paragraph about the S-shaped loss curve is super interesting. As far as I can see they don't speculate on reasons for it, and IMO it's super unintuitive.\n\nI'd be very interested in finding out more about that.",
      "Very interesting that they discussed alternatives to ternary (which is {-1, 0 ,1} like {-1, 1}, {0, 1} and {-2, -1, 0, 1, 2}.\n\nCurious if there are other models (then LLMs) where it would be useful to have a larger set of values but still don't need FP8 or FP16 precision.\n\n>Scaling is one of the primary goals of our research on 1-bit LLMs, as we eventually need to scale up the model size (and training tokens) to train practical LLMs.\n\nIt seems this research group isn't done yet on this topic.",
      "Still waiting for larger model results ðŸ˜­",
      "Just curious. Won't having zero make most equations zero? And if we don't have an operation that can change 0 to something else won't most operations get stuck at the 0 position or the transformers wont use them?\n\n\nWhat if we used an anti zero to make it so the zero could be turned into a one.\n\n\nOr we could use the imaginary number system\n\xa0\n1xi=i\n\xa0ixi=-1\n-1xi=-i\n-ixi=1\n\n\nSince there is 4 symbols 2 bits could be used and i and -i would be replaced with 0 so it would be tje same as the 1.6 bit system except 0's wouldnt be permanent."
    ]
  },
  {
    "title": "Blossom V5 is here!",
    "url": "https://www.reddit.com/r/LocalLLaMA/comments/1bk03p7/blossom_v5_is_here/",
    "comments": [
      "I tried the 14b, it was pretty decent. Probably the best 14b finetune so far (but sadly there aren't many to compare to). There are still some 7/10.7b models I prefer more but it looks like it's getting there.\n\nPS Where does the 9b land on the openllm leaderboard? I see the 7b and 14b there. Would be cool to see the 9b there too. Really interesting seeing all these models do against each other after being trained on the same datasets.",
      "All the data used for training were distilled from gpt-4-0125-preview; therefore, this model's style is similar to GPT-4 Turbo.",
      "Thank you for your interest! Currently, the 9b model is still in the evaluation queue on the openllm leaderboard, and it may take some time. Additionally, in my opinion, the openllm leaderboard has actually lost its reference value (because too many people use the evaluation dataset to train models).",
      "I have to agree with that opinion, but the next best thing, chat arena only allows so many models."
    ]
  }
]
