## [TinyDolphin 2.8 1.1b](https://huggingface.co/Crataco/TinyDolphin-2.8-1.1b-imatrix-GGUF)

- "Takes about ~700MB RAM and tested on my Pi 4 with 2 gigs of RAM. Hallucinates a lot, but works for basic conversation."
- This project seems to be a general-purpose model that is optimized for low memory usage, making it accessible for users with limited resources.

## [Dolphin 2.6 Phi-2](https://huggingface.co/TheBloke/dolphin-2_6-phi-2-GGUF)

- "Takes over ~2GB RAM and tested on my 3GB 32-bit phone via llama.cpp on Termux."
- This model is another general-purpose model, designed for mobile devices with limited resources, specifically 32-bit systems.

## [Nous Hermes Mistral 7B DPO](https://huggingface.co/Crataco/Nous-Hermes-2-Mistral-7B-DPO-imatrix-GGUF)

- "Takes about ~4-5GB RAM depending on context length. Works on my laptop with 8GB RAM."
- This project is a 7B parameter model that balances performance and memory usage, making it suitable for laptops and other devices with limited resources.

## [Nous Hermes 2 SOLAR 10.7B](https://huggingface.co/TheBloke/Nous-Hermes-2-SOLAR-10.7B-GGUF)

- "Takes about ~6-8GB RAM depending on context length. Works on my server PCs and my primary PC (16GB RAM, 4GB VRAM)."
- This model is a larger, high-performance project aimed at users with more powerful hardware.

## [Nous Hermes 2 Mixtral 8x7B DPO](https://huggingface.co/mradermacher/Nous-Hermes-2-Mixtral-8x7B-DPO-i1-GGUF)

- "At IQ3_S, it can run on a laptop with 16GB RAM and 8GB VRAM with 10-11 layers offloaded at 4096 ctx, but I recall it's slightly slower than Q3_K_S (which I had a more consistent ~4.4 tokens/sec with)."
- This model is a high-performance, mixed-precision project, targeting users with more advanced hardware.

## [Kunoichi-DPO-v2-7B](https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF)

- "I mostly look for strong character card adherence, system prompt following, response formatting, general coherence and models that will just go along with the most hardcore NSFW roleplay without resistance."
- This project is designed for roleplaying, specifically for users interested in adult-themed interactions.

## [Fimbulvetr-11B-v2](https://huggingface.co/mradermacher/Fimbulvetr-11B-v2-i1-GGUF)

- "I haven't tested it nearly as much as Kunoichi, so I can't vouch for it, but I hear a lot of great things about it!"
- This model is a larger, less-tested project that has received positive feedback from users.

## [BagelMIsteryTour-v2-8x7B](https://huggingface.co/ycros/BagelMIsteryTour-v2-8x7B-GGUF)

- "probably the best RP model I've ever ran since it hits a great balance of prose and intelligence."
- This project is a roleplay model focused on balancing prose and intelligence for users.

## [ollama](https://github.com/ollama/ollama)

- A terminal client for running models, designed for quick Q&A sessions and easy model deployment.

## [llama.cpp](https://github.com/ggerganov/llama.cpp)

- A terminal client for running models, compatible with various systems and devices.

## [SillyTavern](https://github.com/SillyTavern/SillyTavern) with [KoboldCpp](https://github.com/LostRuins/koboldcpp)

- A frontend for roleplay, often used with models hosted from another PC or for local models.

## [wolfram/miquliz-120b-v2.0](https://huggingface.co/wolfram/miquliz-120b-v2.0)

- "almost exclusively since making it. And I just uploaded additional imatrix GGUF quants today, from IQ1_S to IQ4_XS and in-between (even at 2-bit with IQ2_XS it works great)."
- This project is a large, 120 billion parameter model optimized for various quantization levels, providing flexibility for users.

## [Mistral 7B Instruct v0.2 at Q8\*\_0](https://huggingface.co/TheBloke/Nous-Hermes-2-Mixtral-8x7B-DPO-i1-GGUF)

- "The best all-round daily task: full 32k contexts, fast, affordable, good foreign language, somewhat uncensored, and good for RAG tasks."
- This project is a high-performance, general-purpose model designed for daily tasks, providing extensive context capabilities and foreign language support.

## [Mixtral-Instruct](https://huggingface.co/TheBloke/Nous-Hermes-2-Mixtral-8x7B-DPO-i1-GGUF)

- "I can't find anything I like better than Mixtral-Instruct. I asked a similar question [here](https://www.reddit.com/r/LocalLLaMA/comments/1b9e47s/favorite_mixtral_variant/?utm_source=share&utm_medium=mweb3x&utm_name=mweb3xcss&utm_term=1&utm_content=share_button) a few weeks ago."
- This model is a highly regarded, general-purpose project, well-regarded by users for its performance.

## [Goliath 120b](https://huggingface.co/mradermacher/goliath-120b-gguf)

- "creative prose still makes me unable to live without it. Even though Miqu-120B and Miquliz-120B have appeared."
- This project is a large, creative writing model, preferred by users for its prose capabilities.

## [Blossom V5](https://huggingface.co/TheBloke/Nous-Hermes-2-Mixtral-8x7B-DPO-i1-GGUF)

- "I tried the 14b, it was pretty decent. Probably the best 14b finetune so far (but sadly there aren't many to compare to)."
- This project is a 14 billion parameter model, showcasing the latest improvements in the Blossom series.
