Section 1: List of AI projects with associated links

```
1. [TinyDolphin 2.8 1.1B](https://huggingface.co/Crataco/TinyDolphin-2.8-1.1b-imatrix-GGUF)
2. [Dolphin 2.6 Phi-2](https://huggingface.co/TheBloke/dolphin-2_6-phi-2-GGUF)
3. [Nous Hermes Mistral 7B DPO](https://huggingface.co/Crataco/Nous-Hermes-2-Mistral-7B-DPO-imatrix-GGUF)
4. [Nous Hermes 2 SOLAR 10.7B](https://huggingface.co/TheBloke/Nous-Hermes-2-SOLAR-10.7B-GGUF)
5. [Nous Hermes 2 Mixtral 8x7B DPO](https://huggingface.co/mradermacher/Nous-Hermes-2-Mixtral-8x7B-DPO-i1-GGUF)
6. [Kunoichi-DPO-v2-7B](https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF)
7. [Erosumika](https://huggingface.co/Lewdiculous/Erosumika-7B-GGUF-IQ-Imatrix)
8. [Fimbulvetr-11B-v2](https://huggingface.co/mradermacher/Fimbulvetr-11B-v2-i1-GGUF)
9. [BagelMIsteryTour-v2-8x7B](https://huggingface.co/ycros/BagelMIsteryTour-v2-8x7B-GGUF)
10. [ollama](https://github.com/ollama/ollama)
11. [llama.cpp](https://github.com/ggerganov/llama.cpp)
12. [SillyTavern](https://github.com/SillyTavern/SillyTavern)
13. [KoboldCpp](https://github.com/LostRuins/koboldcpp)
14. [Westlake-10.7B-v2](https://huggingface.co/models?search=Westlake-10.7B-v2)
15. [Noromaid 20B](https://huggingface.co/models?search=Noromaid+20B)
16. [EstopianMaid 13B](https://huggingface.co/models?search=EstopianMaid+13B)
17. [Noromaid-0.4-Mixtral-8x7B-ZLoss](https://huggingface.co/models?search=Noromaid-0.4-Mixtral-8x7B-ZLoss)
18. [MiquMaid](https://huggingface.co/models?search=MiquMaid)
19. [Midnight-Rose](https://huggingface.co/models?search=Midnight-Rose)
20. [Midnight-Miqu](https://huggingface.co/models?search=Midnight-Miqu)
21. [wolfram/miquliz-120b-v2.0](https://huggingface.co/wolfram/miquliz-120b-v2.0)
22. [mistral_7b_instruct_v0.2_DARE](https://huggingface.co/models?search=mistral_7b_instruct_v0.2_DARE)
23. [Endevor/InfinityRP-v1-7B](https://huggingface.co/Endevor/InfinityRP-v1-7B)
24. [Lewdiculous/InfinityRP-v1-7B-GGUF-IQ-Imatrix](https://huggingface.co/Lewdiculous/InfinityRP-v1-7B-GGUF-IQ-Imatrix)
25. [ChaoticNeutrals/BuRP_7B](https://huggingface.co/ChaoticNeutrals/BuRP_7B)
26. [Lewdiculous/BuRP_7B-GGUF-IQ-Imatrix](https://huggingface.co/Lewdiculous/BuRP_7B-GGUF-IQ-Imatrix)
27. [ChaoticNeutrals/Layris_9B](https://huggingface.co/ChaoticNeutrals/Layris_9B/)
28. [Lewdiculous/Layris_9B-GGUF-IQ-Imatrix](https://huggingface.co/Lewdiculous/Layris_9B-GGUF-IQ-Imatrix)
29. [Nitral-AI/Infinitely-Laydiculous-7B](https://huggingface.co/Nitral-AI/Infinitely-Laydiculous-7B)
30. [Lewdiculous/Infinitely-Laydiculous-7B-GGUF-IQ-Imatrix](https://huggingface.co/Lewdiculous/Infinitely-Laydiculous-7B-GGUF-IQ-Imatrix)
31. [SanjiWatsuki/Kunoichi-DPO-v2-7B](https://huggingface.co/SanjiWatsuki/Kunoichi-DPO-v2-7B)
32. [Lewdiculous/Kunoichi-DPO-v2-7B-GGUF-Imatrix](https://huggingface.co/Lewdiculous/Kunoichi-DPO-v2-7B-GGUF-Imatrix)
33. [l3utterfly/mistral-7b-v0.1-layla-v4](https://huggingface.co/l3utterfly/mistral-7b-v0.1-layla-v4/)
34. [Lewdiculous/mistral-7b-v0.1-layla-v4-GGUF-IQ-Imatrix](https://huggingface.co/Lewdiculous/mistral-7b-v0.1-layla-v4-GGUF-IQ-Imatrix)
35. [Nitral-AI/Kunocchini-7b-128k-test](https://huggingface.co/Nitral-AI/Kunocchini-7b-128k-test)
36. [Lewdiculous/Kunocchini-7b-128k-test-GGUF-Imatrix](https://huggingface.co/Lewdiculous/Kunocchini-7b-128k-test-GGUF-Imatrix)
37. [wolfram/miquliz-120b-v2.0-GGUF](https://huggingface.co/wolfram/miquliz-120b-v2.0-GGUF)
```

Section 2: Report on each project

## [TinyDolphin 2.8 1.1B](https://huggingface.co/Crataco/TinyDolphin-2.8-1.1b-imatrix-GGUF)

- "Takes about ~700MB RAM and tested on my Pi 4 with 2 gigs of RAM. Hallucinates a lot, but works for basic conversation."
- This small 1.1B model is notable for its ability to run on low-resource devices like the Raspberry Pi, which aligns with the trend of making large language models more accessible on consumer hardware.

## [Dolphin 2.6 Phi-2](https://huggingface.co/TheBloke/dolphin-2_6-phi-2-GGUF)

- "Takes over ~2GB RAM and tested on my 3GB 32-bit phone via llama.cpp on Termux."
- Similar to TinyDolphin, this 2.7B model demonstrates the push to run language models on mobile and embedded devices with limited RAM.

## [Nous Hermes Mistral 7B DPO](https://huggingface.co/Crataco/Nous-Hermes-2-Mistral-7B-DPO-imatrix-GGUF)

- No direct comments, but part of the Mistral family of models which has garnered interest.
- Represents the continued improvements and variations on decent-sized (~7B parameter) open language models.

## [Nous Hermes 2 SOLAR 10.7B](https://huggingface.co/TheBloke/Nous-Hermes-2-SOLAR-10.7B-GGUF)

- No direct comments.
- Another variation in the Nous Hermes family, showcasing the proliferation of alternative model families and versions.

## [Nous Hermes 2 Mixtral 8x7B DPO](https://huggingface.co/mradermacher/Nous-Hermes-2-Mixtral-8x7B-DPO-i1-GGUF)

- "At IQ3_S, it can run on a laptop with 16GB RAM and 8GB VRAM with 10-11 layers offloaded at 4096 ctx, but I recall it's slightly slower than Q3_K_S (which I had a more consistent ~4.4 tokens/sec with)."
- Highlights the performance tradeoffs and quantization levels needed to run larger composition models like this 8x7B on consumer hardware.

## [Kunoichi-DPO-v2-7B](https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF)

- "Great all around choice. Widely recommended by many users. Punches above what you'd expect."
- A popular and well-regarded 7B model, demonstrating the continued relevance of decent-sized models in certain use cases.

## [Erosumika](https://huggingface.co/Lewdiculous/Erosumika-7B-GGUF-IQ-Imatrix)

- "I **love** [Erosumika](https://huggingface.co/Lewdiculous/Erosumika-7B-GGUF-IQ-Imatrix), which sacrifices the logical yet synthetic GPT dataset for something more organic, similar to older models like Noromaid, Pygmalion 6B, and AI Dungeon 2."
- Interesting for taking a different approach to training data compared to more synthetic models, aiming for a more natural output.

## [Fimbulvetr-11B-v2](https://huggingface.co/mradermacher/Fimbulvetr-11B-v2-i1-GGUF)

- "I haven't tested it nearly as much as Kunoichi, so I can't vouch for it, but I hear a lot of great things about it!"
- Another relatively large (~11B) model that has garnered positive reception, though with limited first-hand experience noted.

## [BagelMIsteryTour-v2-8x7B](https://huggingface.co/ycros/BagelMIsteryTour-v2-8x7B-GGUF)

- "Probably the best RP model I've ever ran since it hits a great balance of prose and intelligence."
- Highlighted as an exceptional roleplay model balancing quality writing with strong performance.

## [ollama](https://github.com/ollama/ollama)

- "Terminal client. I use it for quick Q&A. Automatically offloads GPU layers, easy to download and get a model running, etc. But it [lags behind upstream llama.cpp](https://github.com/ollama/ollama/pull/1825)."
- A terminal-based tool for running language models, noted for its ease of use but behind on some upstream changes.

## [llama.cpp](https://github.com/ggerganov/llama.cpp)

- "Terminal client. With cmake, I was able to compile the latest commit (c47cf41) on my 32-bit Android phone."
- Another terminal client, highlighting the interest in running LLMs on lower-powered devices like phones.

## [SillyTavern](https://github.com/SillyTavern/SillyTavern)

- No direct comments, but mentioned as a frontend used in combination with KoboldCpp.
- Likely popular for its UI and roleplaying capabilities.

## [KoboldCpp](https://github.com/LostRuins/koboldcpp)

- No direct comments, but used as a backend with SillyTavern.
- Another codebase aimed at deploying and interacting with LLMs.

## [Westlake-10.7B-v2](https://huggingface.co/models?search=Westlake-10.7B-v2)

- "The newcomer to the dirty games and fits in as little as 8GB. Almost anyone with a mid-spec gaming rig can run this well and get their fix, and competes very well with the classic 70B+ models, which is nothing short of amazing."
- A relatively compact (~11B) model focused on adult/NSFW content that is seen as punching above its weight.

## [Noromaid 20B](https://huggingface.co/models?search=Noromaid+20B)

- "Anything with Noromaid in it is a staple of rip your clothes off style raunch, a few flavors are worth mentioning. **Noromaid 20B, EstopianMaid 13B, Noromaid-0.4-Mixtral-8x7B-ZLoss**, and the new **MiquMaid** variants will do their worst to you with even the slightest suggestion."
- The Noromaid family of models is highlighted for its reputation in adult/NSFW content generation.

## [EstopianMaid 13B](https://huggingface.co/models?search=EstopianMaid+13B)

- See the quote for Noromaid 20B.

## [Noromaid-0.4-Mixtral-8x7B-ZLoss](https://huggingface.co/models?search=Noromaid-0.4-Mixtral-8x7B-ZLoss)

- See the quote for Noromaid 20B.

## [MiquMaid](https://huggingface.co/models?search=MiquMaid)

- See the quote for Noromaid 20B.

## [Midnight-Rose](https://huggingface.co/models?search=Midnight-Rose)

- "For a more intelligent good time with a slower burn, and if you have lots of VRAM (48GB recommended), consider **Midnight-Rose** or **Midnight-Miqu** (less smutty and more smutty, respectively), in their 70B or 103B forms."
- The Midnight-Rose and Midnight-Miqu models are promoted as high-quality options for more literary/intellectual adult content, but requiring significant VRAM.

## [Midnight-Miqu](https://huggingface.co/models?search=Midnight-Miqu)

- See the quote for Midnight-Rose.

## [wolfram/miquliz-120b-v2.0](https://huggingface.co/wolfram/miquliz-120b-v2.0)

- "I'm obviously partial, but I've been running [wolfram/miquliz-120b-v2.0](https://huggingface.co/wolfram/miquliz-120b-v2.0) almost exclusively since making it."
- A large 120B model that the creator is "partial" to and has been using extensively.

## [wolfram/miquliz-120b-v2.0-GGUF](https://huggingface.co/wolfram/miquliz-120b-v2.0-GGUF)

- "And I just uploaded [additional imatrix GGUF
