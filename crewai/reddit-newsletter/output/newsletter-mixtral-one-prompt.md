## [TinyDolphin 2.8 1.1b](https://huggingface.co/Crataco/TinyDolphin-2.8-1.1b-imatrix-GGUF)

- "Takes about ~700MB RAM and tested on my Pi 4 with 2 gigs of RAM. Hallucinates a lot, but works for basic conversation."
- This project is a general-purpose model that is optimized for low memory usage, making it accessible for users with limited resources.

## [Dolphin 2.6 Phi-2](https://huggingface.co/TheBloke/dolphin-2_6-phi-2-GGUF)

- "Takes over ~2GB RAM and tested on my 3GB 32-bit phone via llama.cpp on Termux."
- This project is a general-purpose model that is compatible with 32-bit systems, expanding its accessibility.

## [Nous Hermes Mistral 7B DPO](https://huggingface.co/Crataco/Nous-Hermes-2-Mistral-7B-DPO-imatrix-GGUF)

- "Takes about ~4-5GB RAM depending on context length. Works on my laptop with 8GB RAM."
- This project is a general-purpose model that is optimized for longer context lengths, making it suitable for more complex conversations.

## [Nous Hermes 2 SOLAR 10.7B](https://huggingface.co/TheBloke/Nous-Hermes-2-SOLAR-10.7B-GGUF)

- "Takes about ~6-8GB RAM depending on context length. Works on my server PCs and my primary PC (16GB RAM, 4GB VRAM)."
- This project is a high-capacity model that can handle extensive contexts, making it suitable for resource-intensive applications.

## [Nous Hermes 2 Mixtral 8x7B DPO](https://huggingface.co/mradermacher/Nous-Hermes-2-Mixtral-8x7B-DPO-i1-GGUF)

- "At IQ3_S, it can run on a laptop with 16GB RAM and 8GB VRAM with 10-11 layers offloaded at 4096 ctx, but I recall it's slightly slower than Q3_K_S (which I had a more consistent ~4.4 tokens/sec with)."
- This project is a high-capacity model that is optimized for offloading layers, making it suitable for users with limited VRAM.

## [Kunoichi-DPO-v2-7B](https://huggingface.co/brittlewis12/Kunoichi-DPO-v2-7B-GGUF)

- "I love [Erosumika](https://huggingface.co/Lewdiculous/Erosumika-7B-GGUF-IQ-Imatrix), which sacrifices the logical yet synthetic GPT dataset for something more organic, similar to older models like Noromaid, Pygmalion 6B, and AI Dungeon 2."
- This project is a roleplay model that prioritizes organic responses, making it suitable for users seeking a more natural interaction.

## [Fimbulvetr-11B-v2](https://huggingface.co/mradermacher/Fimbulvetr-11B-v2-i1-GGUF)

- "I haven't tested it nearly as much as Kunoichi, so I can't vouch for it, but I hear a lot of great things about it!"
- This project is a high-capacity roleplay model that has received positive feedback from users, indicating its potential as a reliable tool.

## [BagelMIsteryTour-v2-8x7B](https://huggingface.co/ycros/BagelMIsteryTour-v2-8x7B-GGUF)

- "probably the best RP model I've ever ran since it hits a great balance of prose and intelligence."
- This project is a roleplay model that balances prose and intelligence, making it suitable for users seeking a more comprehensive interaction.

## [ollama](https://github.com/ollama/ollama)

- "terminal client. I use it for quick Q&A. Automatically offloads GPU layers, easy to download and get a model running, etc. But it [lags behind upstream llama.cpp](https://github.com/ollama/ollama/pull/1825)."
- This project is a terminal client that offloads GPU layers, making it accessible for users with limited resources.

## [llama.cpp](https://github.com/ggerganov/llama.cpp)

- "terminal client. With cmake, I was able to compile the latest commit (c47cf41) on my 32-bit Android phone."
- This project is a terminal client that is compatible with 32-bit systems, expanding its accessibility.

## [SillyTavern](https://github.com/SillyTavern/SillyTavern) with [KoboldCpp](https://github.com/LostRuins/koboldcpp)

- "For roleplay, or just because of its fancy interface, usually if I'm using a model hosted from another PC, want to do a roleplay, or use its built-in RAG capability (vector storage)."
- This project is a roleplay model with a fancy interface and built-in RAG capability, making it suitable for users seeking a more immersive interaction.

These projects highlight the latest advancements in AI technology, with a particular focus on optimizing models for low memory usage, longer contexts, and offloading layers. Additionally, several roleplay models prioritize organic and natural responses, making them suitable for users seeking a more interactive experience. Terminal clients that offload GPU layers and are compatible with 32-bit systems expand the accessibility of AI technology.
